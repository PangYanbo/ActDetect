{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c25dcec-9b11-4c05-9252-a6044ec96b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda54216-6882-4c65-8c30-212c189f709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import ACTIVITIES, ABBR, TZ_PARIS, TZ_LONDON\n",
    "from src.viz_style import apply_nature_style\n",
    "\n",
    "apply_nature_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb665ac2-5151-4849-91b2-34fb4d6ea1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils_time import to_local_time_series, split_cross_midnight, week_start_monday\n",
    "from src.utils_split import split_users_by_hash\n",
    "from src.regularity import regularity_report, summarize_reg, compute_user_hex_stats, infer_home_work_anchors, make_hex_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151d8b27-b904-4272-81aa-f3554b297acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "OUT_DATA = ROOT / \"outputs\" / \"data\"\n",
    "OUT_MODELS = ROOT / \"outputs\" / \"models\"\n",
    "OUT_FIG = ROOT / \"outputs\" / \"figures\"\n",
    "OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load stays\n",
    "train = pd.read_parquet(OUT_DATA / \"paris_stays_train.parquet\")\n",
    "valid = pd.read_parquet(OUT_DATA / \"paris_stays_valid.parquet\")\n",
    "for d in [train, valid]:\n",
    "    d[\"user_id\"] = d[\"user_id\"].astype(str)\n",
    "    d[\"start_time\"] = pd.to_datetime(d[\"start_time\"])\n",
    "    d[\"end_time\"] = pd.to_datetime(d[\"end_time\"])\n",
    "    d[\"duration_min\"] = pd.to_numeric(d[\"duration_min\"], errors=\"coerce\")\n",
    "    d[\"hex_id\"] = d[\"hex_id\"].astype(str).replace({\"\": np.nan, \"nan\": np.nan})\n",
    "    d[\"y_true\"] = d[\"y_true\"].astype(str)\n",
    "    d.dropna(subset=[\"hex_id\",\"start_time\",\"end_time\",\"duration_min\",\"y_true\"], inplace=True)\n",
    "\n",
    "# load Huff POI\n",
    "poi = pd.read_parquet(OUT_DATA / \"paris_poi_huff_k4_b1.5.parquet\")\n",
    "poi[\"hex_id\"] = poi[\"hex_id\"].astype(str)\n",
    "\n",
    "POI_COLS = [\"poi_edu_cnt\",\"poi_health_cnt\",\"poi_retail_cnt\",\"poi_leisure_cnt\",\n",
    "            \"poi_transport_cnt\",\"poi_accom_cnt\",\"poi_office_cnt\",\"poi_total_cnt\"]\n",
    "for c in POI_COLS:\n",
    "    if c not in poi.columns:\n",
    "        poi[c] = 0\n",
    "\n",
    "# load GBDT\n",
    "gbdt = joblib.load(OUT_MODELS / \"paris_gbdt_baseline.joblib\")\n",
    "print(\"Loaded model:\", type(gbdt))\n",
    "\n",
    "# --- rebuild features (copy from 06.5) ---\n",
    "import h3, math\n",
    "\n",
    "def cell_to_latlon(cell):\n",
    "    if hasattr(h3, \"cell_to_latlng\"):\n",
    "        lat, lon = h3.cell_to_latlng(cell)\n",
    "    else:\n",
    "        lat, lon = h3.h3_to_geo(cell)\n",
    "    return float(lat), float(lon)\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    p1, p2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2-lat1)\n",
    "    dlmb = math.radians(lon2-lon1)\n",
    "    a = math.sin(dphi/2)**2 + math.cos(p1)*math.cos(p2)*math.sin(dlmb/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "all_hex = pd.Index(pd.concat([train[\"hex_id\"], valid[\"hex_id\"]]).unique()).astype(str)\n",
    "centroids = {h: cell_to_latlon(h) for h in all_hex}\n",
    "\n",
    "def infer_anchors_and_home_area(stays_df, k_home_area=3):\n",
    "    d = stays_df.copy().sort_values([\"user_id\",\"start_time\"])\n",
    "    d[\"date\"] = d[\"start_time\"].dt.date\n",
    "    mid = d[\"start_time\"] + pd.to_timedelta(d[\"duration_min\"]/2, unit=\"m\")\n",
    "    hh = mid.dt.hour + mid.dt.minute/60.0\n",
    "    night = (hh >= 20) | (hh < 6)\n",
    "    weekday = (d[\"start_time\"].dt.weekday < 5)\n",
    "    workhour = weekday & (hh >= 9) & (hh < 17)\n",
    "    d[\"night_dwell\"] = np.where(night, d[\"duration_min\"], 0.0)\n",
    "    d[\"work_dwell\"]  = np.where(workhour, d[\"duration_min\"], 0.0)\n",
    "\n",
    "    g = d.groupby([\"user_id\",\"hex_id\"], as_index=False).agg(\n",
    "        night_dwell=(\"night_dwell\",\"sum\"),\n",
    "        work_dwell=(\"work_dwell\",\"sum\"),\n",
    "        visit_days=(\"date\", lambda x: x.nunique()),\n",
    "        dwell=(\"duration_min\",\"sum\"),\n",
    "    )\n",
    "\n",
    "    home_hex = g.sort_values([\"user_id\",\"night_dwell\"], ascending=[True,False]) \\\n",
    "                .drop_duplicates(\"user_id\")[[\"user_id\",\"hex_id\"]].rename(columns={\"hex_id\":\"home_hex\"})\n",
    "    work_hex = g.merge(home_hex, on=\"user_id\", how=\"left\")\n",
    "    work_hex = work_hex[work_hex[\"hex_id\"] != work_hex[\"home_hex\"]]\n",
    "    work_hex = work_hex.sort_values([\"user_id\",\"work_dwell\"], ascending=[True,False]) \\\n",
    "                       .drop_duplicates(\"user_id\")[[\"user_id\",\"hex_id\"]].rename(columns={\"hex_id\":\"work_hex\"})\n",
    "\n",
    "    home_area = g[g[\"night_dwell\"] > 0].sort_values([\"user_id\",\"night_dwell\"], ascending=[True,False]) \\\n",
    "                  .groupby(\"user_id\").head(k_home_area)\n",
    "    home_area_lookup = home_area.groupby(\"user_id\")[\"hex_id\"].apply(lambda x: set(x.astype(str))).to_dict()\n",
    "\n",
    "    anchors = home_hex.merge(work_hex, on=\"user_id\", how=\"left\")\n",
    "    anchors[\"home_hex\"] = anchors[\"home_hex\"].astype(str)\n",
    "    anchors[\"work_hex\"] = anchors[\"work_hex\"].astype(str)\n",
    "    return anchors, home_area_lookup\n",
    "\n",
    "anchors_train, home_area_train = infer_anchors_and_home_area(train, k_home_area=3)\n",
    "anchors_valid, home_area_valid = infer_anchors_and_home_area(valid, k_home_area=3)\n",
    "home_valid = dict(zip(anchors_valid[\"user_id\"], anchors_valid[\"home_hex\"]))\n",
    "work_valid = dict(zip(anchors_valid[\"user_id\"], anchors_valid[\"work_hex\"]))\n",
    "\n",
    "def add_poi_features(df, poi_df):\n",
    "    d = df.merge(poi_df[[\"hex_id\"]+POI_COLS], on=\"hex_id\", how=\"left\").fillna(0)\n",
    "    sem = d[\"poi_edu_cnt\"] + d[\"poi_health_cnt\"] + d[\"poi_retail_cnt\"] + d[\"poi_leisure_cnt\"]\n",
    "    d[\"sem_total\"] = sem\n",
    "    d[\"edu_ratio\"] = d[\"poi_edu_cnt\"]/(sem+1.0)\n",
    "    d[\"health_ratio\"] = d[\"poi_health_cnt\"]/(sem+1.0)\n",
    "    d[\"retail_ratio\"] = d[\"poi_retail_cnt\"]/(sem+1.0)\n",
    "    d[\"leisure_ratio\"] = d[\"poi_leisure_cnt\"]/(sem+1.0)\n",
    "    return d\n",
    "\n",
    "def add_time_duration_features(df):\n",
    "    d = df.copy()\n",
    "    st = d[\"start_time\"]\n",
    "    d[\"hour\"] = st.dt.hour\n",
    "    d[\"dow\"] = st.dt.weekday\n",
    "    d[\"is_weekday\"] = (d[\"dow\"] < 5).astype(int)\n",
    "    d[\"log_dur\"] = np.log1p(d[\"duration_min\"].astype(float))\n",
    "    return d\n",
    "\n",
    "def add_distance_features(df, home_lookup, work_lookup, home_area_lookup):\n",
    "    d = df.copy()\n",
    "    latlon = d[\"hex_id\"].map(lambda h: centroids.get(str(h), (np.nan, np.nan)))\n",
    "    d[\"lat\"] = [x[0] for x in latlon]\n",
    "    d[\"lon\"] = [x[1] for x in latlon]\n",
    "\n",
    "    def anchor_latlon(u, mp):\n",
    "        hh = mp.get(u, None)\n",
    "        return centroids.get(str(hh), (np.nan, np.nan))\n",
    "\n",
    "    home_ll = d[\"user_id\"].map(lambda u: anchor_latlon(u, home_lookup))\n",
    "    work_ll = d[\"user_id\"].map(lambda u: anchor_latlon(u, work_lookup))\n",
    "    d[\"home_lat\"] = [x[0] for x in home_ll]; d[\"home_lon\"] = [x[1] for x in home_ll]\n",
    "    d[\"work_lat\"] = [x[0] for x in work_ll]; d[\"work_lon\"] = [x[1] for x in work_ll]\n",
    "\n",
    "    d[\"dist_to_home_km\"] = [\n",
    "        haversine_km(a,b,c,e) if np.isfinite(a) and np.isfinite(c) else np.nan\n",
    "        for a,b,c,e in zip(d[\"lat\"], d[\"lon\"], d[\"home_lat\"], d[\"home_lon\"])\n",
    "    ]\n",
    "    d[\"dist_to_work_km\"] = [\n",
    "        haversine_km(a,b,c,e) if np.isfinite(a) and np.isfinite(c) else np.nan\n",
    "        for a,b,c,e in zip(d[\"lat\"], d[\"lon\"], d[\"work_lat\"], d[\"work_lon\"])\n",
    "    ]\n",
    "\n",
    "    def dist_to_home_area_min(u, lat, lon):\n",
    "        hs = home_area_lookup.get(u, None)\n",
    "        if not hs:\n",
    "            return np.nan\n",
    "        best = np.inf\n",
    "        for hh in hs:\n",
    "            ll = centroids.get(str(hh))\n",
    "            if ll is None: \n",
    "                continue\n",
    "            best = min(best, haversine_km(lat, lon, ll[0], ll[1]))\n",
    "        return best if np.isfinite(best) else np.nan\n",
    "\n",
    "    d[\"dist_to_home_area_min_km\"] = [\n",
    "        dist_to_home_area_min(u, la, lo) if np.isfinite(la) else np.nan\n",
    "        for u, la, lo in zip(d[\"user_id\"], d[\"lat\"], d[\"lon\"])\n",
    "    ]\n",
    "\n",
    "    d[\"near_home_0p5km\"] = (d[\"dist_to_home_km\"] <= 0.5).astype(int)\n",
    "    d[\"near_work_0p5km\"] = (d[\"dist_to_work_km\"] <= 0.5).astype(int)\n",
    "    return d\n",
    "\n",
    "def add_prev_distance(df):\n",
    "    d = df.sort_values([\"user_id\",\"start_time\"]).copy()\n",
    "    prev_hex = d.groupby(\"user_id\")[\"hex_id\"].shift(1)\n",
    "    prev_ll = prev_hex.map(lambda h: centroids.get(str(h), (np.nan, np.nan)))\n",
    "    prev_lat = [x[0] for x in prev_ll]; prev_lon = [x[1] for x in prev_ll]\n",
    "    d[\"dist_prev_km\"] = [\n",
    "        haversine_km(la, lo, pla, plo) if np.isfinite(la) and np.isfinite(pla) else np.nan\n",
    "        for la,lo,pla,plo in zip(d[\"lat\"], d[\"lon\"], prev_lat, prev_lon)\n",
    "    ]\n",
    "    return d\n",
    "\n",
    "FEATURES = [\n",
    "    \"hour\",\"dow\",\"is_weekday\",\"duration_min\",\"log_dur\",\n",
    "    \"poi_edu_cnt\",\"poi_health_cnt\",\"poi_retail_cnt\",\"poi_leisure_cnt\",\"poi_total_cnt\",\n",
    "    \"edu_ratio\",\"health_ratio\",\"retail_ratio\",\"leisure_ratio\",\n",
    "    \"dist_to_home_km\",\"dist_to_work_km\",\"dist_to_home_area_min_km\",\"near_home_0p5km\",\"near_work_0p5km\",\n",
    "    \"dist_prev_km\",\n",
    "]\n",
    "\n",
    "def build_features(stays):\n",
    "    d = stays.copy()\n",
    "    d = add_time_duration_features(d)\n",
    "    d = add_poi_features(d, poi)\n",
    "    d = add_distance_features(d, home_valid, work_valid, home_area_valid)\n",
    "    d = add_prev_distance(d)\n",
    "    return d\n",
    "\n",
    "X_valid_df = build_features(valid)\n",
    "Xva = X_valid_df[FEATURES].replace([np.inf,-np.inf], np.nan)\n",
    "yva = X_valid_df[\"y_true\"].astype(str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08404100-e280-4e7e-a9ae-b57138ac622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT ACC: 0.6265980541931334 Macro: 0.4907338508592717 Macro excl HOME: 0.4275618679574135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HOME      0.877     0.863     0.870      4153\n",
      "        WORK      0.799     0.646     0.714      2566\n",
      "       STUDY      0.158     0.392     0.225       237\n",
      "    PURCHASE      0.518     0.583     0.548      1832\n",
      "     LEISURE      0.464     0.512     0.487      1290\n",
      "      HEALTH      0.148     0.528     0.231       301\n",
      "       OTHER      0.508     0.278     0.360      2058\n",
      "\n",
      "    accuracy                          0.627     12437\n",
      "   macro avg      0.496     0.543     0.491     12437\n",
      "weighted avg      0.673     0.627     0.638     12437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "pred_gbdt = gbdt.predict(Xva)\n",
    "acc = accuracy_score(yva, pred_gbdt)\n",
    "macro = f1_score(yva, pred_gbdt, average=\"macro\", labels=ACTIVITIES)\n",
    "macro_weak = f1_score(yva, pred_gbdt, average=\"macro\", labels=[\"WORK\",\"STUDY\",\"PURCHASE\",\"LEISURE\",\"HEALTH\",\"OTHER\"])\n",
    "\n",
    "print(\"GBDT ACC:\", acc, \"Macro:\", macro, \"Macro excl HOME:\", macro_weak)\n",
    "print(classification_report(yva, pred_gbdt, labels=ACTIVITIES, digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a75f9aa-00f4-40fe-a7df-a8e76b1600d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse TIME_BINS from notebook 06; if not, define quickly\n",
    "TIME_BINS = [(0,360),(360,600),(600,960),(960,1200),(1200,1440)]\n",
    "\n",
    "def time_bin_id(minute):\n",
    "    for i,(a,b) in enumerate(TIME_BINS):\n",
    "        if a <= minute < b: return i\n",
    "    return -1\n",
    "\n",
    "act2i = {a:i for i,a in enumerate(ACTIVITIES)}\n",
    "i2act = {i:a for a,i in act2i.items()}\n",
    "K = len(ACTIVITIES)\n",
    "\n",
    "def split_by_gap(df_u, break_gap_hours=8):\n",
    "    thr = break_gap_hours * 3600.0\n",
    "    g = df_u.sort_values(\"start_time\")\n",
    "    if len(g)==0: return []\n",
    "    idxs = list(g.index)\n",
    "    parts=[]; cur=[idxs[0]]\n",
    "    for a,b in zip(idxs[:-1], idxs[1:]):\n",
    "        gap = (g.loc[b,\"start_time\"] - g.loc[a,\"end_time\"]).total_seconds()\n",
    "        if gap > thr:\n",
    "            parts.append(g.loc[cur].copy()); cur=[b]\n",
    "        else:\n",
    "            cur.append(b)\n",
    "    parts.append(g.loc[cur].copy())\n",
    "    return parts\n",
    "\n",
    "def build_A_dict_from_stays(stays_df, alpha=1.0, rho=0.3, break_gap_hours=8):\n",
    "    df = stays_df.sort_values([\"user_id\",\"start_time\"]).copy()\n",
    "\n",
    "    def ctx(dt):\n",
    "        w = int(dt.weekday() >= 5)\n",
    "        tb = time_bin_id(int(dt.hour*60 + dt.minute))\n",
    "        return (w, tb)\n",
    "\n",
    "    C_global = np.zeros((K,K), float)\n",
    "    C_bucket = {}\n",
    "\n",
    "    for u, g in df.groupby(\"user_id\", sort=False):\n",
    "        parts = split_by_gap(g, break_gap_hours=break_gap_hours)\n",
    "        for seq in parts:\n",
    "            y = seq[\"y_true\"].astype(str).values\n",
    "            for i in range(len(seq)-1):\n",
    "                a, b = y[i], y[i+1]\n",
    "                if a not in act2i or b not in act2i:\n",
    "                    continue\n",
    "                dt = seq.iloc[i][\"start_time\"]\n",
    "                w, tb = ctx(dt)\n",
    "                if tb < 0:\n",
    "                    continue\n",
    "                if (w,tb) not in C_bucket:\n",
    "                    C_bucket[(w,tb)] = np.zeros((K,K), float)\n",
    "                ii, jj = act2i[a], act2i[b]\n",
    "                C_bucket[(w,tb)][ii, jj] += 1.0\n",
    "                C_global[ii, jj] += 1.0\n",
    "\n",
    "    def row_norm(C):\n",
    "        A = C + alpha\n",
    "        A = A / A.sum(axis=1, keepdims=True)\n",
    "        return A\n",
    "\n",
    "    A_global = row_norm(C_global)\n",
    "    A_dict = {}\n",
    "    for w in [0,1]:\n",
    "        for tb in range(len(TIME_BINS)):\n",
    "            C = C_bucket.get((w,tb))\n",
    "            if C is None:\n",
    "                A_dict[(w,tb)] = A_global.copy()\n",
    "            else:\n",
    "                A = row_norm(C)\n",
    "                A = (1-rho)*A + rho*A_global\n",
    "                A = A / A.sum(axis=1, keepdims=True)\n",
    "                A_dict[(w,tb)] = A\n",
    "    return A_dict\n",
    "\n",
    "A_dict = build_A_dict_from_stays(train, alpha=1.0, rho=0.3, break_gap_hours=8)\n",
    "\n",
    "def A_getter(seq, t):\n",
    "    dt = seq.iloc[t][\"start_time\"]\n",
    "    w = int(dt.weekday() >= 5)\n",
    "    tb = time_bin_id(int(dt.hour*60 + dt.minute))\n",
    "    return A_dict[(w,tb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6af9403-dda5-4f74-a62b-6c28e2700131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build TRAIN features (same pipeline as valid)\n",
    "# X_train_df = build_features(train)  # build_features() you defined in Cell 1\n",
    "# Xtr = X_train_df[FEATURES].replace([np.inf, -np.inf], np.nan)\n",
    "# ytr = X_train_df[\"y_true\"].astype(str).values\n",
    "\n",
    "# print(\"Xtr shape:\", Xtr.shape, \"train rows:\", len(X_train_df), \"train users:\", X_train_df[\"user_id\"].nunique())\n",
    "\n",
    "# proba_tr = gbdt.predict_proba(Xtr)   # Xtr 是 train 的 features\n",
    "# classes = list(gbdt.classes_)\n",
    "# idx_map = [classes.index(a) for a in ACTIVITIES]\n",
    "# proba_tr = proba_tr[:, idx_map]      # align to ACTIVITIES order\n",
    "\n",
    "\n",
    "# train_feat = X_train_df.sort_values([\"user_id\",\"start_time\"]).copy()\n",
    "# train_feat[\"proba_row\"] = list(proba_tr)\n",
    "\n",
    "\n",
    "# def build_A_dict_from_proba(train_feat, alpha=1.0, rho=0.3, break_gap_hours=8):\n",
    "#     # train_feat: sorted by user_id,start_time, has start_time,end_time, proba_row (len K)\n",
    "#     C_global = np.zeros((K,K), float)\n",
    "#     C_bucket = {}\n",
    "\n",
    "#     def ctx(dt):\n",
    "#         w = int(dt.weekday() >= 5)\n",
    "#         tb = time_bin_id(int(dt.hour*60 + dt.minute))\n",
    "#         return (w, tb)\n",
    "\n",
    "#     for u, g in train_feat.groupby(\"user_id\", sort=False):\n",
    "#         parts = split_by_gap(g, break_gap_hours=break_gap_hours)\n",
    "#         for seq in parts:\n",
    "#             for t in range(len(seq)-1):\n",
    "#                 dt = seq.iloc[t][\"start_time\"]\n",
    "#                 w, tb = ctx(dt)\n",
    "#                 if tb < 0: \n",
    "#                     continue\n",
    "#                 if (w,tb) not in C_bucket:\n",
    "#                     C_bucket[(w,tb)] = np.zeros((K,K), float)\n",
    "\n",
    "#                 p = np.array(seq.iloc[t][\"proba_row\"], dtype=float)\n",
    "#                 q = np.array(seq.iloc[t+1][\"proba_row\"], dtype=float)\n",
    "#                 C = np.outer(p, q)           # expected transition counts\n",
    "#                 C_bucket[(w,tb)] += C\n",
    "#                 C_global += C\n",
    "\n",
    "#     def row_norm(C):\n",
    "#         A = C + alpha\n",
    "#         A = A / A.sum(axis=1, keepdims=True)\n",
    "#         return A\n",
    "\n",
    "#     A_global = row_norm(C_global)\n",
    "#     A_dict = {}\n",
    "#     for w in [0,1]:\n",
    "#         for tb in range(len(TIME_BINS)):\n",
    "#             C = C_bucket.get((w,tb))\n",
    "#             if C is None:\n",
    "#                 A_dict[(w,tb)] = A_global.copy()\n",
    "#             else:\n",
    "#                 A = row_norm(C)\n",
    "#                 A = (1-rho)*A + rho*A_global\n",
    "#                 A = A / A.sum(axis=1, keepdims=True)\n",
    "#                 A_dict[(w,tb)] = A\n",
    "#     return A_dict\n",
    "\n",
    "# A_dict_soft = build_A_dict_from_proba(train_feat, alpha=1.0, rho=0.3, break_gap_hours=8)\n",
    "\n",
    "# def A_getter(seq, t):\n",
    "#     dt = seq.iloc[t][\"start_time\"]\n",
    "#     w = int(dt.weekday() >= 5)\n",
    "#     tb = time_bin_id(int(dt.hour*60 + dt.minute))\n",
    "#     return A_dict_soft[(w,tb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e55888f0-6b36-4245-ad05-b0a93e1d53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid (GBDT emission + HMM) ACC: 0.6690520221918469 Macro: 0.5128403271320069 Macro excl HOME: 0.45004974257133634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HOME      0.889     0.890     0.890      4153\n",
      "        WORK      0.752     0.769     0.760      2566\n",
      "       STUDY      0.224     0.194     0.208       237\n",
      "    PURCHASE      0.515     0.635     0.569      1832\n",
      "     LEISURE      0.486     0.511     0.498      1290\n",
      "      HEALTH      0.219     0.346     0.268       301\n",
      "       OTHER      0.500     0.329     0.397      2058\n",
      "\n",
      "    accuracy                          0.669     12437\n",
      "   macro avg      0.512     0.525     0.513     12437\n",
      "weighted avg      0.670     0.669     0.665     12437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def logsumexp(v):\n",
    "    m = np.max(v)\n",
    "    return float(m + np.log(np.sum(np.exp(v - m))))\n",
    "\n",
    "def viterbi_decode(logB, seq, self_loop_bonus=0.2):\n",
    "    eps = 1e-12\n",
    "    T, K0 = logB.shape\n",
    "    log_pi = np.log(np.full(K0, 1.0/K0))\n",
    "    dp = np.full((T,K0), -np.inf)\n",
    "    bp = np.zeros((T,K0), dtype=int)\n",
    "    dp[0] = log_pi + logB[0]\n",
    "    bp[0] = -1\n",
    "\n",
    "    for t in range(1,T):\n",
    "        A = A_getter(seq, t-1)\n",
    "        logA = np.log(A + eps)\n",
    "        for j in range(K0):\n",
    "            scores = dp[t-1] + logA[:,j] + self_loop_bonus*(np.arange(K0)==j)\n",
    "            i_star = int(np.argmax(scores))\n",
    "            bp[t,j] = i_star\n",
    "            dp[t,j] = scores[i_star] + logB[t,j]\n",
    "\n",
    "    last = int(np.argmax(dp[T-1]))\n",
    "    path=[last]\n",
    "    for t in range(T-1,0,-1):\n",
    "        last = bp[t,last]\n",
    "        path.append(last)\n",
    "    return np.array(path[::-1], dtype=int)\n",
    "\n",
    "# proba -> logB aligned to ACTIVITIES order\n",
    "proba = gbdt.predict_proba(Xva)  # shape (N, n_classes_in_model)\n",
    "classes = list(gbdt.classes_)\n",
    "idx_map = [classes.index(a) for a in ACTIVITIES]  # assume all present\n",
    "proba_aligned = proba[:, idx_map]\n",
    "\n",
    "T=0.5\n",
    "logB_all = np.log(np.clip(proba_aligned, 1e-12, 1.0)) / T\n",
    "\n",
    "# decode per user segment\n",
    "df = X_valid_df.copy().sort_values([\"user_id\",\"start_time\"])\n",
    "df[\"logB_row\"] = list(logB_all)  # store row vectors\n",
    "\n",
    "df[\"y_pred_hybrid\"] = None\n",
    "\n",
    "for u, g in df.groupby(\"user_id\", sort=False):\n",
    "    parts = split_by_gap(g, break_gap_hours=8)\n",
    "    for seq in parts:\n",
    "        logB = np.stack(seq[\"logB_row\"].values, axis=0)\n",
    "        path = viterbi_decode(logB, seq, self_loop_bonus=0.1)\n",
    "        df.loc[seq.index, \"y_pred_hybrid\"] = [i2act[int(i)] for i in path]\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "y_pred_hybrid = df[\"y_pred_hybrid\"].astype(str).values\n",
    "acc_h = accuracy_score(yva, y_pred_hybrid)\n",
    "macro_h = f1_score(yva, y_pred_hybrid, average=\"macro\", labels=ACTIVITIES)\n",
    "macro_weak_h = f1_score(yva, y_pred_hybrid, average=\"macro\", labels=[\"WORK\",\"STUDY\",\"PURCHASE\",\"LEISURE\",\"HEALTH\",\"OTHER\"])\n",
    "\n",
    "print(\"Hybrid (GBDT emission + HMM) ACC:\", acc_h, \"Macro:\", macro_h, \"Macro excl HOME:\", macro_weak_h)\n",
    "print(classification_report(yva, y_pred_hybrid, labels=ACTIVITIES, digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e59dcd4e-c7e9-4900-9606-8105857b482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/pang/Codes/GISRUK/outputs/data/cm_hybrid_gbdt_hmm_row_norm.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(yva, y_pred_hybrid, labels=ACTIVITIES)\n",
    "cmn = cm / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
    "\n",
    "cm_csv = OUT_DATA / \"cm_hybrid_gbdt_hmm_row_norm.csv\"\n",
    "np.savetxt(cm_csv, cmn, delimiter=\",\", fmt=\"%.6f\")\n",
    "print(\"Saved:\", cm_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce06dcba-9b0b-4612-a822-89773037372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_row_norm_cm(y_true, y_pred, out_csv):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cmn = cm / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
    "    np.savetxt(out_csv, cmn, delimiter=\",\", fmt=\"%.6f\")\n",
    "    print(\"Saved:\", out_csv)\n",
    "    return cmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e7b3c14-25c5-4ffd-b23d-dd87a17fe201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/pang/Codes/GISRUK/outputs/data/cm_hybrid_row_norm.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.89044065, 0.02311582, 0.00120395, 0.03106188, 0.02167108,\n",
       "        0.00722369, 0.02528293],\n",
       "       [0.03546376, 0.76890101, 0.04988309, 0.05144193, 0.03312549,\n",
       "        0.020265  , 0.04091972],\n",
       "       [0.03375527, 0.58649789, 0.19409283, 0.02109705, 0.07594937,\n",
       "        0.01265823, 0.07594937],\n",
       "       [0.05131004, 0.04257642, 0.0010917 , 0.63537118, 0.07914847,\n",
       "        0.06058952, 0.12991266],\n",
       "       [0.05116279, 0.09379845, 0.01085271, 0.16124031, 0.51085271,\n",
       "        0.04651163, 0.1255814 ],\n",
       "       [0.03322259, 0.09966777, 0.        , 0.27242525, 0.08305648,\n",
       "        0.34551495, 0.16611296],\n",
       "       [0.09426628, 0.09135083, 0.00485909, 0.26287658, 0.16229349,\n",
       "        0.05539359, 0.32896016]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_row_norm_cm(yva, y_pred_hybrid, OUT_DATA / \"cm_hybrid_row_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36f5f048-6b4e-473b-a8a5-528e2b55b4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/pang/Codes/GISRUK/outputs/data/paris_valid_pred_hybrid.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "OUT_DATA = Path(\".\")\n",
    "OUT_DATA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_out = X_valid_df[[\"user_id\",\"start_time\",\"end_time\",\"hex_id\",\"duration_min\",\"y_true\"]].copy()\n",
    "df_out[\"y_pred\"] = y_pred_hybrid\n",
    "df_out.to_parquet(OUT_DATA / \"paris_valid_pred_hybrid.parquet\", index=False)\n",
    "print(\"Saved:\", OUT_DATA / \"paris_valid_pred_hybrid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d4059-c7a9-4015-834c-e9f4be915e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
