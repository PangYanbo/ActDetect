{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c25dcec-9b11-4c05-9252-a6044ec96b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda54216-6882-4c65-8c30-212c189f709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import ACTIVITIES, ABBR, TZ_PARIS, TZ_LONDON\n",
    "from src.viz_style import apply_nature_style\n",
    "\n",
    "apply_nature_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb665ac2-5151-4849-91b2-34fb4d6ea1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils_time import to_local_time_series, split_cross_midnight, week_start_monday\n",
    "from src.utils_split import split_users_by_hash\n",
    "from src.regularity import regularity_report, summarize_reg, compute_user_hex_stats, infer_home_work_anchors, make_hex_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37c1673-9d67-4fec-9099-f0ace4a1201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train stays: 49803 users: 2482\n",
      "Valid stays: 12437 users: 621\n",
      "POI rows: 29628\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "OUT_DATA = ROOT / \"outputs\" / \"data\"\n",
    "OUT_MODELS = ROOT / \"outputs\" / \"models\"\n",
    "OUT_FIG = ROOT / \"outputs\" / \"figures\"\n",
    "OUT_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train = pd.read_parquet(OUT_DATA / \"paris_stays_train.parquet\")\n",
    "valid = pd.read_parquet(OUT_DATA / \"paris_stays_valid.parquet\")\n",
    "\n",
    "for d in [train, valid]:\n",
    "    d[\"user_id\"] = d[\"user_id\"].astype(str)\n",
    "    d[\"start_time\"] = pd.to_datetime(d[\"start_time\"])\n",
    "    d[\"end_time\"] = pd.to_datetime(d[\"end_time\"])\n",
    "    d[\"duration_min\"] = pd.to_numeric(d[\"duration_min\"], errors=\"coerce\")\n",
    "    d[\"hex_id\"] = d[\"hex_id\"].astype(str).replace({\"\": np.nan, \"nan\": np.nan})\n",
    "    d[\"y_true\"] = d[\"y_true\"].astype(str)\n",
    "    d.dropna(subset=[\"hex_id\",\"start_time\",\"end_time\",\"duration_min\",\"y_true\"], inplace=True)\n",
    "\n",
    "poi = pd.read_parquet(OUT_DATA / \"paris_poi_huff_k4_b1.5.parquet\")\n",
    "poi[\"hex_id\"] = poi[\"hex_id\"].astype(str)\n",
    "\n",
    "POI_COLS = [\"poi_edu_cnt\",\"poi_health_cnt\",\"poi_retail_cnt\",\"poi_leisure_cnt\",\n",
    "            \"poi_transport_cnt\",\"poi_accom_cnt\",\"poi_office_cnt\",\"poi_total_cnt\"]\n",
    "for c in POI_COLS:\n",
    "    if c not in poi.columns:\n",
    "        poi[c] = 0\n",
    "\n",
    "print(\"Train stays:\", len(train), \"users:\", train[\"user_id\"].nunique())\n",
    "print(\"Valid stays:\", len(valid), \"users:\", valid[\"user_id\"].nunique())\n",
    "print(\"POI rows:\", len(poi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f018de5-ef9a-4beb-b73e-9f7cc6e87381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids computed: 29628\n"
     ]
    }
   ],
   "source": [
    "import h3\n",
    "import math\n",
    "\n",
    "def cell_to_latlon(cell):\n",
    "    if hasattr(h3, \"cell_to_latlng\"):\n",
    "        lat, lon = h3.cell_to_latlng(cell)\n",
    "    else:\n",
    "        lat, lon = h3.h3_to_geo(cell)\n",
    "    return float(lat), float(lon)\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    p1, p2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2-lat1)\n",
    "    dlmb = math.radians(lon2-lon1)\n",
    "    a = math.sin(dphi/2)**2 + math.cos(p1)*math.cos(p2)*math.sin(dlmb/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "# Precompute centroids for all hex in train+valid\n",
    "all_hex = pd.Index(pd.concat([train[\"hex_id\"], valid[\"hex_id\"]]).unique()).astype(str)\n",
    "centroids = {h: cell_to_latlon(h) for h in all_hex}\n",
    "print(\"Centroids computed:\", len(centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c125071-80a0-4d54-aed2-69d88f5752e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchors train users: 2482 valid users: 621\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def infer_anchors_and_home_area(stays_df, k_home_area=3):\n",
    "    d = stays_df.copy().sort_values([\"user_id\",\"start_time\"])\n",
    "    d[\"date\"] = d[\"start_time\"].dt.date\n",
    "\n",
    "    mid = d[\"start_time\"] + pd.to_timedelta(d[\"duration_min\"]/2, unit=\"m\")\n",
    "    hh = mid.dt.hour + mid.dt.minute/60.0\n",
    "    night = (hh >= 20) | (hh < 6)\n",
    "    weekday = (d[\"start_time\"].dt.weekday < 5)\n",
    "    workhour = weekday & (hh >= 9) & (hh < 17)\n",
    "\n",
    "    d[\"night_dwell\"] = np.where(night, d[\"duration_min\"], 0.0)\n",
    "    d[\"work_dwell\"]  = np.where(workhour, d[\"duration_min\"], 0.0)\n",
    "\n",
    "    # per user-hex agg\n",
    "    g = d.groupby([\"user_id\",\"hex_id\"], as_index=False).agg(\n",
    "        night_dwell=(\"night_dwell\",\"sum\"),\n",
    "        work_dwell=(\"work_dwell\",\"sum\"),\n",
    "        visit_days=(\"date\", lambda x: x.nunique()),\n",
    "        dwell=(\"duration_min\",\"sum\"),\n",
    "    )\n",
    "\n",
    "    home_hex = g.sort_values([\"user_id\",\"night_dwell\"], ascending=[True,False]) \\\n",
    "                .drop_duplicates(\"user_id\")[[\"user_id\",\"hex_id\"]].rename(columns={\"hex_id\":\"home_hex\"})\n",
    "    work_hex = g.merge(home_hex, on=\"user_id\", how=\"left\")\n",
    "    work_hex = work_hex[work_hex[\"hex_id\"] != work_hex[\"home_hex\"]]\n",
    "    work_hex = work_hex.sort_values([\"user_id\",\"work_dwell\"], ascending=[True,False]) \\\n",
    "                       .drop_duplicates(\"user_id\")[[\"user_id\",\"hex_id\"]].rename(columns={\"hex_id\":\"work_hex\"})\n",
    "\n",
    "    # home area: top-K by night_dwell\n",
    "    home_area = g[g[\"night_dwell\"] > 0].sort_values([\"user_id\",\"night_dwell\"], ascending=[True,False]) \\\n",
    "                  .groupby(\"user_id\").head(k_home_area)\n",
    "    home_area_lookup = home_area.groupby(\"user_id\")[\"hex_id\"].apply(lambda x: set(x.astype(str))).to_dict()\n",
    "\n",
    "    anchors = home_hex.merge(work_hex, on=\"user_id\", how=\"left\")\n",
    "    anchors[\"home_hex\"] = anchors[\"home_hex\"].astype(str)\n",
    "    anchors[\"work_hex\"] = anchors[\"work_hex\"].astype(str)\n",
    "    return anchors, home_area_lookup\n",
    "\n",
    "anchors_train, home_area_train = infer_anchors_and_home_area(train, k_home_area=3)\n",
    "anchors_valid, home_area_valid = infer_anchors_and_home_area(valid, k_home_area=3)\n",
    "\n",
    "home_train = dict(zip(anchors_train[\"user_id\"], anchors_train[\"home_hex\"]))\n",
    "work_train = dict(zip(anchors_train[\"user_id\"], anchors_train[\"work_hex\"]))\n",
    "home_valid = dict(zip(anchors_valid[\"user_id\"], anchors_valid[\"home_hex\"]))\n",
    "work_valid = dict(zip(anchors_valid[\"user_id\"], anchors_valid[\"work_hex\"]))\n",
    "\n",
    "print(\"Anchors train users:\", len(home_train), \"valid users:\", len(home_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168ab628-f767-472d-bab0-2c89001a7dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'start_time', 'end_time', 'duration_min', 'hex_id', 'y_true',\n",
      "       'hour', 'dow', 'is_weekday', 'log_dur', 'poi_edu_cnt', 'poi_health_cnt',\n",
      "       'poi_retail_cnt', 'poi_leisure_cnt', 'poi_transport_cnt',\n",
      "       'poi_accom_cnt', 'poi_office_cnt', 'poi_total_cnt', 'sem_total',\n",
      "       'edu_ratio', 'health_ratio', 'retail_ratio', 'leisure_ratio', 'lat',\n",
      "       'lon', 'home_lat', 'home_lon', 'work_lat', 'work_lon',\n",
      "       'dist_to_home_km', 'dist_to_work_km', 'dist_to_home_area_min_km',\n",
      "       'near_home_0p5km', 'near_work_0p5km', 'dist_prev_km'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def add_poi_features(df, poi_df):\n",
    "    d = df.merge(poi_df[[\"hex_id\"]+POI_COLS], on=\"hex_id\", how=\"left\").fillna(0)\n",
    "    sem = d[\"poi_edu_cnt\"] + d[\"poi_health_cnt\"] + d[\"poi_retail_cnt\"] + d[\"poi_leisure_cnt\"]\n",
    "    d[\"sem_total\"] = sem\n",
    "    d[\"edu_ratio\"] = d[\"poi_edu_cnt\"]/(sem+1.0)\n",
    "    d[\"health_ratio\"] = d[\"poi_health_cnt\"]/(sem+1.0)\n",
    "    d[\"retail_ratio\"] = d[\"poi_retail_cnt\"]/(sem+1.0)\n",
    "    d[\"leisure_ratio\"] = d[\"poi_leisure_cnt\"]/(sem+1.0)\n",
    "    return d\n",
    "\n",
    "def add_time_duration_features(df):\n",
    "    d = df.copy()\n",
    "    st = d[\"start_time\"]\n",
    "    d[\"hour\"] = st.dt.hour\n",
    "    d[\"dow\"] = st.dt.weekday\n",
    "    d[\"is_weekday\"] = (d[\"dow\"] < 5).astype(int)\n",
    "    d[\"log_dur\"] = np.log1p(d[\"duration_min\"].astype(float))\n",
    "    return d\n",
    "\n",
    "def add_distance_features(df, home_lookup, work_lookup, home_area_lookup):\n",
    "    d = df.copy()\n",
    "    # hex centroid\n",
    "    latlon = d[\"hex_id\"].map(lambda h: centroids.get(str(h), (np.nan, np.nan)))\n",
    "    d[\"lat\"] = [x[0] for x in latlon]\n",
    "    d[\"lon\"] = [x[1] for x in latlon]\n",
    "\n",
    "    # anchor centroids\n",
    "    def anchor_latlon(u, anchor_map):\n",
    "        h = anchor_map.get(u, None)\n",
    "        return centroids.get(str(h), (np.nan, np.nan))\n",
    "\n",
    "    home_ll = d[\"user_id\"].map(lambda u: anchor_latlon(u, home_lookup))\n",
    "    work_ll = d[\"user_id\"].map(lambda u: anchor_latlon(u, work_lookup))\n",
    "    d[\"home_lat\"] = [x[0] for x in home_ll]; d[\"home_lon\"] = [x[1] for x in home_ll]\n",
    "    d[\"work_lat\"] = [x[0] for x in work_ll]; d[\"work_lon\"] = [x[1] for x in work_ll]\n",
    "\n",
    "    # distances\n",
    "    d[\"dist_to_home_km\"] = [\n",
    "        haversine_km(a,b,c,e) if np.isfinite(a) and np.isfinite(c) else np.nan\n",
    "        for a,b,c,e in zip(d[\"lat\"], d[\"lon\"], d[\"home_lat\"], d[\"home_lon\"])\n",
    "    ]\n",
    "    d[\"dist_to_work_km\"] = [\n",
    "        haversine_km(a,b,c,e) if np.isfinite(a) and np.isfinite(c) else np.nan\n",
    "        for a,b,c,e in zip(d[\"lat\"], d[\"lon\"], d[\"work_lat\"], d[\"work_lon\"])\n",
    "    ]\n",
    "\n",
    "    # distance to home_area (min)\n",
    "    def dist_to_home_area_min(u, lat, lon):\n",
    "        hs = home_area_lookup.get(u, None)\n",
    "        if not hs:\n",
    "            return np.nan\n",
    "        best = np.inf\n",
    "        for hh in hs:\n",
    "            ll = centroids.get(str(hh))\n",
    "            if ll is None: \n",
    "                continue\n",
    "            best = min(best, haversine_km(lat, lon, ll[0], ll[1]))\n",
    "        return best if np.isfinite(best) else np.nan\n",
    "\n",
    "    d[\"dist_to_home_area_min_km\"] = [\n",
    "        dist_to_home_area_min(u, la, lo) if np.isfinite(la) else np.nan\n",
    "        for u, la, lo in zip(d[\"user_id\"], d[\"lat\"], d[\"lon\"])\n",
    "    ]\n",
    "\n",
    "    # binary proximity flags\n",
    "    d[\"near_home_0p5km\"] = (d[\"dist_to_home_km\"] <= 0.5).astype(int)\n",
    "    d[\"near_work_0p5km\"] = (d[\"dist_to_work_km\"] <= 0.5).astype(int)\n",
    "\n",
    "    return d\n",
    "\n",
    "def add_prev_distance(df):\n",
    "    d = df.sort_values([\"user_id\",\"start_time\"]).copy()\n",
    "    prev_hex = d.groupby(\"user_id\")[\"hex_id\"].shift(1)\n",
    "    prev_ll = prev_hex.map(lambda h: centroids.get(str(h), (np.nan, np.nan)))\n",
    "    prev_lat = [x[0] for x in prev_ll]; prev_lon = [x[1] for x in prev_ll]\n",
    "\n",
    "    d[\"dist_prev_km\"] = [\n",
    "        haversine_km(la, lo, pla, plo) if np.isfinite(la) and np.isfinite(pla) else np.nan\n",
    "        for la,lo,pla,plo in zip(d[\"lat\"], d[\"lon\"], prev_lat, prev_lon)\n",
    "    ]\n",
    "    return d\n",
    "\n",
    "def build_features(stays, home_lookup, work_lookup, home_area_lookup):\n",
    "    d = stays.copy()\n",
    "    d = add_time_duration_features(d)\n",
    "    d = add_poi_features(d, poi)\n",
    "    d = add_distance_features(d, home_lookup, work_lookup, home_area_lookup)\n",
    "    d = add_prev_distance(d)\n",
    "    return d\n",
    "\n",
    "X_train_df = build_features(train, home_train, work_train, home_area_train)\n",
    "X_valid_df = build_features(valid, home_valid, work_valid, home_area_valid)\n",
    "\n",
    "print(X_train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb311e9f-ada2-4cc9-950e-89cd01005afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT ACC: 0.6265980541931334 Macro: 0.4907338508592717 Macro excl HOME: 0.4275618679574135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        HOME      0.877     0.863     0.870      4153\n",
      "        WORK      0.799     0.646     0.714      2566\n",
      "       STUDY      0.158     0.392     0.225       237\n",
      "    PURCHASE      0.518     0.583     0.548      1832\n",
      "     LEISURE      0.464     0.512     0.487      1290\n",
      "      HEALTH      0.148     0.528     0.231       301\n",
      "       OTHER      0.508     0.278     0.360      2058\n",
      "\n",
      "    accuracy                          0.627     12437\n",
      "   macro avg      0.496     0.543     0.491     12437\n",
      "weighted avg      0.673     0.627     0.638     12437\n",
      "\n",
      "Saved: /Users/pang/Codes/GISRUK/outputs/models/paris_gbdt_baseline.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "FEATURES = [\n",
    "    # time + duration\n",
    "    \"hour\",\"dow\",\"is_weekday\",\"duration_min\",\"log_dur\",\n",
    "    # POI counts\n",
    "    \"poi_edu_cnt\",\"poi_health_cnt\",\"poi_retail_cnt\",\"poi_leisure_cnt\",\"poi_total_cnt\",\n",
    "    # POI ratios\n",
    "    \"edu_ratio\",\"health_ratio\",\"retail_ratio\",\"leisure_ratio\",\n",
    "    # distances\n",
    "    \"dist_to_home_km\",\"dist_to_work_km\",\"dist_to_home_area_min_km\",\"near_home_0p5km\",\"near_work_0p5km\",\n",
    "    # prev distance\n",
    "    \"dist_prev_km\",\n",
    "]\n",
    "\n",
    "def prep_xy(df):\n",
    "    X = df[FEATURES].replace([np.inf,-np.inf], np.nan)\n",
    "    # HistGB can handle NaN; for MLP we will impute later\n",
    "    y = df[\"y_true\"].astype(str).values\n",
    "    return X, y\n",
    "\n",
    "Xtr, ytr = prep_xy(X_train_df)\n",
    "Xva, yva = prep_xy(X_valid_df)\n",
    "\n",
    "# sample weights to reduce class imbalance impact\n",
    "w = compute_sample_weight(class_weight=\"balanced\", y=ytr)\n",
    "\n",
    "# --- GBDT (fast, strong baseline) ---\n",
    "gbdt = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.08,\n",
    "    max_depth=6,\n",
    "    max_iter=400,\n",
    "    random_state=0\n",
    ")\n",
    "gbdt.fit(Xtr, ytr, sample_weight=w)\n",
    "\n",
    "pred = gbdt.predict(Xva)\n",
    "\n",
    "acc = accuracy_score(yva, pred)\n",
    "macro = f1_score(yva, pred, average=\"macro\", labels=ACTIVITIES)\n",
    "macro_weak = f1_score(yva, pred, average=\"macro\", labels=[\"WORK\",\"STUDY\",\"PURCHASE\",\"LEISURE\",\"HEALTH\",\"OTHER\"])\n",
    "\n",
    "print(\"GBDT ACC:\", acc, \"Macro:\", macro, \"Macro excl HOME:\", macro_weak)\n",
    "print(classification_report(yva, pred, labels=ACTIVITIES, digits=3, zero_division=0))\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(gbdt, OUT_MODELS / \"paris_gbdt_baseline.joblib\")\n",
    "print(\"Saved:\", OUT_MODELS / \"paris_gbdt_baseline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7476afd-41ea-4c59-8686-4281dbe32080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setting</th>\n",
       "      <th>ACC</th>\n",
       "      <th>Macro</th>\n",
       "      <th>Macro_weak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All (+prev-dist)</td>\n",
       "      <td>0.626598</td>\n",
       "      <td>0.490734</td>\n",
       "      <td>0.427562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Base + POI + anchor-dist</td>\n",
       "      <td>0.614939</td>\n",
       "      <td>0.480119</td>\n",
       "      <td>0.418197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Base + POI</td>\n",
       "      <td>0.537750</td>\n",
       "      <td>0.440396</td>\n",
       "      <td>0.390492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Base + anchor-dist</td>\n",
       "      <td>0.573772</td>\n",
       "      <td>0.438313</td>\n",
       "      <td>0.370905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base: time+duration</td>\n",
       "      <td>0.480743</td>\n",
       "      <td>0.392999</td>\n",
       "      <td>0.336348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    setting       ACC     Macro  Macro_weak\n",
       "4          All (+prev-dist)  0.626598  0.490734    0.427562\n",
       "3  Base + POI + anchor-dist  0.614939  0.480119    0.418197\n",
       "1                Base + POI  0.537750  0.440396    0.390492\n",
       "2        Base + anchor-dist  0.573772  0.438313    0.370905\n",
       "0       Base: time+duration  0.480743  0.392999    0.336348"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "WEAK = [\"WORK\",\"STUDY\",\"PURCHASE\",\"LEISURE\",\"HEALTH\",\"OTHER\"]\n",
    "\n",
    "def fit_eval(feature_list, name):\n",
    "    Xtr = X_train_df[feature_list].replace([np.inf,-np.inf], np.nan)\n",
    "    ytr = X_train_df[\"y_true\"].astype(str).values\n",
    "    Xva = X_valid_df[feature_list].replace([np.inf,-np.inf], np.nan)\n",
    "    yva = X_valid_df[\"y_true\"].astype(str).values\n",
    "\n",
    "    w = compute_sample_weight(class_weight=\"balanced\", y=ytr)\n",
    "\n",
    "    clf = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.08, max_depth=6, max_iter=400, random_state=0\n",
    "    )\n",
    "    clf.fit(Xtr, ytr, sample_weight=w)\n",
    "    pred = clf.predict(Xva)\n",
    "\n",
    "    acc = accuracy_score(yva, pred)\n",
    "    macro = f1_score(yva, pred, average=\"macro\", labels=ACTIVITIES)\n",
    "    macro_weak = f1_score(yva, pred, average=\"macro\", labels=WEAK)\n",
    "    return {\"setting\": name, \"ACC\": acc, \"Macro\": macro, \"Macro_weak\": macro_weak}\n",
    "\n",
    "BASE = [\"hour\",\"dow\",\"is_weekday\",\"duration_min\",\"log_dur\"]\n",
    "\n",
    "POI_ONLY = [\"poi_edu_cnt\",\"poi_health_cnt\",\"poi_retail_cnt\",\"poi_leisure_cnt\",\"poi_total_cnt\",\n",
    "            \"edu_ratio\",\"health_ratio\",\"retail_ratio\",\"leisure_ratio\"]\n",
    "\n",
    "ANCHOR_DIST = [\"dist_to_home_km\",\"dist_to_work_km\",\"dist_to_home_area_min_km\",\"near_home_0p5km\",\"near_work_0p5km\"]\n",
    "\n",
    "PREV_DIST = [\"dist_prev_km\"]\n",
    "\n",
    "ABL = []\n",
    "ABL.append(fit_eval(BASE, \"Base: time+duration\"))\n",
    "ABL.append(fit_eval(BASE+POI_ONLY, \"Base + POI\"))\n",
    "ABL.append(fit_eval(BASE+ANCHOR_DIST, \"Base + anchor-dist\"))\n",
    "ABL.append(fit_eval(BASE+POI_ONLY+ANCHOR_DIST, \"Base + POI + anchor-dist\"))\n",
    "ABL.append(fit_eval(BASE+POI_ONLY+ANCHOR_DIST+PREV_DIST, \"All (+prev-dist)\"))\n",
    "\n",
    "pd.DataFrame(ABL).sort_values(\"Macro_weak\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "151d8b27-b904-4272-81aa-f3554b297acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"HOME\",\"WORK\",\"STUDY\",\"PURCHASE\",\"LEISURE\",\"HEALTH\",\"OTHER\"]\n",
    "\n",
    "def save_row_norm_cm(y_true, y_pred, out_csv):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cmn = cm / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
    "    np.savetxt(out_csv, cmn, delimiter=\",\", fmt=\"%.6f\")\n",
    "    print(\"Saved:\", out_csv)\n",
    "    return cmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4688ab9-614b-43a3-8dea-9d6749501db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/pang/Codes/GISRUK/outputs/data/cm_gbdt_row_norm.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.86274982, 0.01854081, 0.00674211, 0.03274741, 0.03106188,\n",
       "        0.02143029, 0.02672767],\n",
       "       [0.03858145, 0.64575214, 0.14497272, 0.04286828, 0.03546376,\n",
       "        0.05884645, 0.0335152 ],\n",
       "       [0.02953586, 0.35443038, 0.39240506, 0.02531646, 0.092827  ,\n",
       "        0.0464135 , 0.05907173],\n",
       "       [0.05076419, 0.02565502, 0.00491266, 0.58296943, 0.07860262,\n",
       "        0.14792576, 0.10917031],\n",
       "       [0.0627907 , 0.05813953, 0.03488372, 0.14341085, 0.51162791,\n",
       "        0.09767442, 0.09147287],\n",
       "       [0.05647841, 0.04318937, 0.00996678, 0.21262458, 0.05980066,\n",
       "        0.5282392 , 0.089701  ],\n",
       "       [0.10009718, 0.05830904, 0.01895044, 0.24003887, 0.1739553 ,\n",
       "        0.13022352, 0.27842566]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_row_norm_cm(yva, pred, OUT_DATA / \"cm_gbdt_row_norm.csv\")  # pred 是 GBDT 的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8ffcf-f26b-4b79-a0af-33b8ebad5405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
