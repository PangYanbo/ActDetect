{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc91697c-0140-4cab-a34f-9ff87bf932eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24da1fa9-164e-49e1-84ff-8de4a4c6135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import folium\n",
    "import h3\n",
    "import math\n",
    "\n",
    "from src.viz_style import apply_nature_style\n",
    "from src.utils_time import to_local_time_series\n",
    "\n",
    "apply_nature_style()\n",
    "\n",
    "TZ_LONDON = \"Europe/London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "951ac805-719d-438b-8f7f-da83a835cd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available prediction files:\n",
      "1W -> ['Rule-based', 'GBDT', 'HMM', 'Hybrid', 'Hybrid(OD-normalized)']\n",
      "1M -> ['GBDT', 'HMM', 'Hybrid']\n",
      "3M -> ['GBDT', 'Hybrid']\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path(\".\")\n",
    "OUT_DATA = ROOT / \"outputs\" / \"data\"\n",
    "OUT_TAB  = ROOT / \"outputs\" / \"tables\"\n",
    "\n",
    "REGISTRY = {\n",
    "    \"1W\": {\n",
    "        \"Rule-based\": OUT_DATA / \"uk_1w_pred_rulebased.parquet\",\n",
    "        \"GBDT\":       OUT_DATA / \"uk_1w_pred_gbdt.parquet\",\n",
    "        \"HMM\":        OUT_DATA / \"uk_1w_pred_hmm.parquet\",\n",
    "        \"Hybrid\":     OUT_DATA / \"uk_1w_pred_hybrid.parquet\",  \n",
    "        \"Hybrid(OD-normalized)\": OUT_DATA / \"uk_1w_pred_hybrid_od.parquet\"\n",
    "    },\n",
    "    \"1M\": {\n",
    "        \"Rule-based\": OUT_DATA / \"uk_1m_pred_rulebased.parquet\",\n",
    "        \"GBDT\":       OUT_DATA / \"uk_1m_pred_gbdt.parquet\",\n",
    "        \"HMM\":        OUT_DATA / \"uk_1m_pred_hmm.parquet\",\n",
    "        \"Hybrid\":     OUT_DATA / \"uk_1m_pred_hybrid.parquet\",\n",
    "    },\n",
    "    \"3M\": {\n",
    "        \"Rule-based\": OUT_DATA / \"uk_3m_pred_rulebased.parquet\",\n",
    "        \"GBDT\":       OUT_DATA / \"uk_3m_pred_gbdt.parquet\",\n",
    "        \"HMM\":        OUT_DATA / \"uk_3m_pred_hmm.parquet\",\n",
    "        \"Hybrid\":     OUT_DATA / \"uk_3m_pred_hybrid.parquet\",\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "def available_models(horizon):\n",
    "    return [m for m, p in REGISTRY[horizon].items() if p.exists()]\n",
    "\n",
    "print(\"Available prediction files:\")\n",
    "for h in [\"1W\",\"1M\",\"3M\"]:\n",
    "    print(h, \"->\", available_models(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "572683f9-b6cf-4ee7-9033-bf7cd099f515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample pack: uk_1w_review_sample_pack.csv | groups: ['entropy_high', 'entropy_low', 'entropy_drop_high']\n",
      "Added group: secondary_night_home_ge60min | n = 13\n",
      "Final groups: ['All users', 'entropy_drop_high', 'entropy_high', 'entropy_low', 'secondary_night_home_ge60min']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Always initialize (prevents NameError even if files missing)\n",
    "group_to_users = {}\n",
    "\n",
    "# 1) Load review sample pack (entropy_high/low/drop)\n",
    "sample_path = OUT_TAB / \"uk_1w_review_sample_pack.csv\"\n",
    "if sample_path.exists():\n",
    "    sample_pack = pd.read_csv(sample_path)\n",
    "    sample_pack[\"user_id\"] = sample_pack[\"user_id\"].astype(str)\n",
    "    for g in sample_pack[\"group\"].unique():\n",
    "        group_to_users[g] = sorted(sample_pack[sample_pack[\"group\"] == g][\"user_id\"].unique())\n",
    "    print(\"Loaded sample pack:\", sample_path.name, \"| groups:\", list(group_to_users.keys()))\n",
    "else:\n",
    "    print(\"No sample pack found:\", sample_path)\n",
    "\n",
    "# 2) Add secondary-night-home group (>=60min)\n",
    "sec_path = OUT_TAB / \"uk_1w_secondary_night_home_users_ge60min.csv\"\n",
    "if sec_path.exists():\n",
    "    sec_users = pd.read_csv(sec_path)[\"user_id\"].astype(str).unique().tolist()\n",
    "    group_to_users[\"secondary_night_home_ge60min\"] = sorted(sec_users)\n",
    "    print(\"Added group: secondary_night_home_ge60min | n =\", len(sec_users))\n",
    "else:\n",
    "    print(\"No secondary-night-home list found:\", sec_path)\n",
    "\n",
    "# Final check\n",
    "print(\"Final groups:\", [\"All users\"] + sorted(group_to_users.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61bc2fcc-05d9-404d-828c-36d5e63910c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cache = {}\n",
    "\n",
    "def load_pred(horizon, model):\n",
    "    key = (horizon, model)\n",
    "    if key in _cache:\n",
    "        return _cache[key]\n",
    "\n",
    "    path = REGISTRY[horizon][model]\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    # standardize columns\n",
    "    if \"user_id\" not in df.columns and \"userid\" in df.columns:\n",
    "        df = df.rename(columns={\"userid\": \"user_id\"})\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "    df[\"start_time\"] = pd.to_datetime(df[\"start_time\"])\n",
    "    df[\"end_time\"] = pd.to_datetime(df[\"end_time\"])\n",
    "    if \"duration_min\" not in df.columns:\n",
    "        df[\"duration_min\"] = (df[\"end_time\"] - df[\"start_time\"]).dt.total_seconds()/60.0\n",
    "    df[\"duration_min\"] = pd.to_numeric(df[\"duration_min\"], errors=\"coerce\").fillna(0.0)\n",
    "    df[\"hex_id\"] = df[\"hex_id\"].astype(str)\n",
    "\n",
    "    # prediction column: unify to \"label\"\n",
    "    if \"label_od\" in df.columns:\n",
    "        df[\"label\"] = df[\"label_od\"].astype(str)\n",
    "    elif \"y_pred\" in df.columns:\n",
    "        df[\"label\"] = df[\"y_pred\"].astype(str)\n",
    "    elif \"y_pred_s4\" in df.columns:\n",
    "        df[\"label\"] = df[\"y_pred_s4\"].astype(str)\n",
    "    elif \"y_pred_s3\" in df.columns:\n",
    "        df[\"label\"] = df[\"y_pred_s3\"].astype(str)\n",
    "    else:\n",
    "        raise ValueError(...)\n",
    "        \n",
    "    # keep only necessary cols (reduce memory)\n",
    "    keep_cols = [\"user_id\",\"start_time\",\"end_time\",\"duration_min\",\"hex_id\",\"label\"]\n",
    "    for c in [\"lat\",\"lon\"]:\n",
    "        if c in df.columns:\n",
    "            keep_cols.append(c)\n",
    "\n",
    "    df = df[keep_cols].copy()\n",
    "    _cache[key] = df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c58a1d-1e88-4914-a06f-c85e25974b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_COL = {\n",
    "    \"HOME\":\"#1f77b4\",\"WORK\":\"#ff7f0e\",\"STUDY\":\"#2ca02c\",\"PURCHASE\":\"#d62728\",\n",
    "    \"LEISURE\":\"#9467bd\",\"HEALTH\":\"#8c564b\",\"OTHER\":\"#e377c2\",\n",
    "}\n",
    "\n",
    "def week_start_monday_local(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    return (ts - pd.Timedelta(days=ts.weekday())).normalize()\n",
    "\n",
    "def filter_user_week(df, user_id, week_start_str, tz=TZ_LONDON):\n",
    "    d = df[df[\"user_id\"] == str(user_id)].copy()\n",
    "    d[\"start_time\"] = to_local_time_series(d[\"start_time\"], tz=tz, assume_utc_if_naive=True)\n",
    "    d[\"end_time\"]   = to_local_time_series(d[\"end_time\"], tz=tz, assume_utc_if_naive=True)\n",
    "    d = d.sort_values(\"start_time\").copy()\n",
    "\n",
    "    ws = pd.to_datetime(week_start_str)\n",
    "    if ws.tzinfo is None:\n",
    "        ws = ws.tz_localize(tz)\n",
    "    we = ws + pd.Timedelta(days=7)\n",
    "\n",
    "    d = d[(d[\"start_time\"] >= ws) & (d[\"start_time\"] < we)].copy()\n",
    "    return d\n",
    "\n",
    "def night_home_share(d_week: pd.DataFrame, label_col=\"label\"):\n",
    "    d = d_week.copy()\n",
    "    mid = d[\"start_time\"] + pd.to_timedelta(d[\"duration_min\"]/2, unit=\"m\")\n",
    "    hh = mid.dt.hour + mid.dt.minute/60.0\n",
    "    night = (hh >= 20) | (hh < 6)\n",
    "    d[\"_night_dwell\"] = np.where(night, d[\"duration_min\"], 0.0)\n",
    "    d[\"_night_home\"] = np.where(night & (d[label_col] == \"HOME\"), d[\"duration_min\"], 0.0)\n",
    "    g = d.groupby(\"user_id\", as_index=False).agg(night_dwell=(\"_night_dwell\",\"sum\"), night_home=(\"_night_home\",\"sum\"))\n",
    "    g[\"night_home_share\"] = g[\"night_home\"] / g[\"night_dwell\"].replace(0, np.nan)\n",
    "    return float(g[\"night_home_share\"].fillna(0.0).mean()) if len(g) else 0.0\n",
    "\n",
    "def tophex_entropy(d_week: pd.DataFrame, label_col=\"label\", top_n=5):\n",
    "    d = d_week.copy()\n",
    "    dwell = d.groupby([\"user_id\",\"hex_id\"], as_index=False)[\"duration_min\"].sum()\n",
    "    dwell[\"rank\"] = dwell.groupby(\"user_id\")[\"duration_min\"].rank(method=\"first\", ascending=False)\n",
    "    top = dwell[dwell[\"rank\"] <= top_n][[\"user_id\",\"hex_id\"]]\n",
    "    d = d.merge(top, on=[\"user_id\",\"hex_id\"], how=\"inner\")\n",
    "\n",
    "    def ent_of_labels(s):\n",
    "        vc = s.value_counts().values.astype(float)\n",
    "        p = vc / vc.sum()\n",
    "        return float(-(p*np.log(np.clip(p,1e-12,1.0))).sum())\n",
    "\n",
    "    rows=[]\n",
    "    for (u,h), g in d.groupby([\"user_id\",\"hex_id\"], sort=False):\n",
    "        rows.append(ent_of_labels(g[label_col].astype(str)))\n",
    "    return float(np.mean(rows)) if rows else 0.0\n",
    "\n",
    "def label_switch_rate(d_week: pd.DataFrame, label_col=\"label\"):\n",
    "    d = d_week.sort_values([\"user_id\",\"start_time\"]).copy()\n",
    "    d[\"date\"] = d[\"start_time\"].dt.date\n",
    "    def switches_one_day(g):\n",
    "        labs = g[label_col].astype(str).values\n",
    "        if len(labs)<=1:\n",
    "            return 0\n",
    "        return int(np.sum(labs[1:] != labs[:-1]))\n",
    "    sw = d.groupby([\"user_id\",\"date\"]).apply(switches_one_day)\n",
    "    return float(sw.mean()) if len(sw) else 0.0\n",
    "\n",
    "def proxy_summary(d_week):\n",
    "    return night_home_share(d_week), tophex_entropy(d_week), label_switch_rate(d_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1356458-4e0d-4515-9e2b-2cdbe28553a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_lanes(day_df):\n",
    "    \"\"\"Greedy lane assignment to avoid overlaps within a day.\"\"\"\n",
    "    day_df = day_df.sort_values(\"start_time\").copy()\n",
    "    lane_ends = []\n",
    "    lane = []\n",
    "    for idx, r in day_df.iterrows():\n",
    "        placed = False\n",
    "        for li, last_end in enumerate(lane_ends):\n",
    "            if r[\"start_time\"] >= last_end:\n",
    "                lane_ends[li] = r[\"end_time\"]\n",
    "                lane.append((idx, li))\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            lane_ends.append(r[\"end_time\"])\n",
    "            lane.append((idx, len(lane_ends)-1))\n",
    "    lane_map = dict(lane)\n",
    "    day_df[\"lane\"] = [lane_map[i] for i in day_df.index]\n",
    "    return day_df, len(lane_ends)\n",
    "\n",
    "\n",
    "def split_cross_midnight_for_ui(d):\n",
    "    out_rows = []\n",
    "    for r in d.itertuples(index=False):\n",
    "        u = str(r.user_id)\n",
    "        st = r.start_time\n",
    "        en = r.end_time\n",
    "        dur = float(r.duration_min)\n",
    "        hx = str(r.hex_id)\n",
    "        lab = str(r.label)\n",
    "        imp = bool(getattr(r, \"imputed_midnight\", False))\n",
    "\n",
    "        if en.date() == st.date():\n",
    "            out_rows.append((u, st, en, dur, hx, lab, imp))\n",
    "            continue\n",
    "\n",
    "        cur = st\n",
    "        while cur.date() < en.date():\n",
    "            midn = (cur.normalize() + pd.Timedelta(days=1))\n",
    "            out_rows.append((u, cur, midn, (midn-cur).total_seconds()/60.0, hx, lab, imp))\n",
    "            cur = midn\n",
    "        out_rows.append((u, cur, en, (en-cur).total_seconds()/60.0, hx, lab, imp))\n",
    "\n",
    "    cols = [\"user_id\",\"start_time\",\"end_time\",\"duration_min\",\"hex_id\",\"label\",\"imputed_midnight\"]\n",
    "    return pd.DataFrame(out_rows, columns=cols)\n",
    "\n",
    "def plot_week_timeline(d_week, title, max_lanes=10, impute_gap_min=30):\n",
    "    \"\"\"\n",
    "    Draw stays by true start/end.\n",
    "    If a stay starts at 00:00 and the previous observed stay ended much earlier (>impute_gap_min),\n",
    "    treat that 00:00 start as imputed and DO NOT draw it (leave blank).\n",
    "    \"\"\"\n",
    "    apply_nature_style()\n",
    "    dow_names = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "\n",
    "    d = d_week.copy()\n",
    "    d = d.sort_values([\"user_id\",\"start_time\"]).copy()\n",
    "\n",
    "    # ---- detect likely day-start imputation ----\n",
    "    d[\"prev_end\"] = d.groupby(\"user_id\")[\"end_time\"].shift(1)\n",
    "    d[\"gap_prev_min\"] = (d[\"start_time\"] - d[\"prev_end\"]).dt.total_seconds()/60.0\n",
    "\n",
    "    # start at midnight?\n",
    "    st = d[\"start_time\"]\n",
    "    is_midnight = (st.dt.hour==0) & (st.dt.minute==0) & (st.dt.second==0)\n",
    "\n",
    "    # if gap from previous end to 00:00 is large -> likely imputed\n",
    "    # (prev_end exists AND gap_prev_min is large positive)\n",
    "    d[\"imputed_midnight\"] = is_midnight & d[\"prev_end\"].notna() & (d[\"gap_prev_min\"] > impute_gap_min)\n",
    "\n",
    "    # keep only needed cols\n",
    "    d = d[[\"user_id\",\"start_time\",\"end_time\",\"duration_min\",\"hex_id\",\"label\",\"imputed_midnight\"]].copy()\n",
    "\n",
    "    # split for per-day plotting\n",
    "    d = split_cross_midnight_for_ui(d)\n",
    "    d[\"dow\"] = d[\"start_time\"].dt.weekday\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4.8))\n",
    "\n",
    "    for dow in range(7):\n",
    "        day = d[d[\"dow\"]==dow].copy()\n",
    "        if len(day)==0:\n",
    "            continue\n",
    "\n",
    "        # assign lanes\n",
    "        day, nlanes = assign_lanes(day)\n",
    "        nlanes = min(nlanes, max_lanes)\n",
    "        lane_h = 0.8 / max(1, nlanes)\n",
    "\n",
    "        # draw in time order; HOME underlay then others (so non-HOME visible)\n",
    "        day_home  = day[day[\"label\"]==\"HOME\"].sort_values(\"start_time\")\n",
    "        day_other = day[day[\"label\"]!=\"HOME\"].sort_values(\"start_time\")\n",
    "\n",
    "        def draw_rows(rows, z, alpha):\n",
    "            for _, r in rows.iterrows():\n",
    "                # ---- skip imputed midnight segments (leave blank) ----\n",
    "                if bool(r[\"imputed_midnight\"]) and r[\"start_time\"].hour == 0:\n",
    "                    continue\n",
    "\n",
    "                lane = int(r[\"lane\"])\n",
    "                if lane >= max_lanes:\n",
    "                    lane = max_lanes - 1\n",
    "\n",
    "                st, en = r[\"start_time\"], r[\"end_time\"]\n",
    "                x0 = st.hour + st.minute/60 + st.second/3600\n",
    "                x1 = en.hour + en.minute/60 + en.second/3600\n",
    "                y = dow + 0.10 + lane * lane_h\n",
    "\n",
    "                col = ACT_COL.get(str(r[\"label\"]), \"#777777\")\n",
    "                ax.plot([x0,x1],[y,y], lw=7, color=col, solid_capstyle=\"butt\",\n",
    "                        alpha=alpha, zorder=z)\n",
    "\n",
    "        draw_rows(day_home,  z=1, alpha=0.95)\n",
    "        draw_rows(day_other, z=3, alpha=1.00)\n",
    "\n",
    "    ax.set_xlim(0,24)\n",
    "    ax.set_xticks(np.arange(0,24,2))\n",
    "    ax.set_xlabel(\"Hour of day (local)\")\n",
    "    ax.set_ylim(0,7)\n",
    "    ax.set_yticks(np.arange(0.5,7.5,1.0))\n",
    "    ax.set_yticklabels(dow_names)\n",
    "    ax.set_title(title)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.tick_params(direction=\"in\")\n",
    "\n",
    "    handles = [Line2D([0],[0], color=ACT_COL[a], lw=7) for a in [\"HOME\",\"WORK\",\"STUDY\",\"PURCHASE\",\"LEISURE\",\"HEALTH\",\"OTHER\"]]\n",
    "    ax.legend(handles, [\"HOME\",\"WORK\",\"STUDY\",\"PURCHASE\",\"LEISURE\",\"HEALTH\",\"OTHER\"],\n",
    "              ncol=4, loc=\"upper center\", bbox_to_anchor=(0.5, -0.18), frameon=False)\n",
    "\n",
    "    plt.tight_layout(rect=[0,0.08,1,1])\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24481ae0-3135-49d9-902d-78b31bafc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h3_to_geojson_polygon(cell):\n",
    "    boundary = h3.cell_to_boundary(cell, geo_json=False) if hasattr(h3, \"cell_to_boundary\") else h3.h3_to_geo_boundary(cell, geo_json=False)\n",
    "    coords = [(lon, lat) for lat, lon in boundary]\n",
    "    coords.append(coords[0])\n",
    "    return [coords]\n",
    "\n",
    "def to_parent(cell, res_parent):\n",
    "    return h3.cell_to_parent(cell, res_parent) if hasattr(h3, \"cell_to_parent\") else h3.h3_to_parent(cell, res_parent)\n",
    "\n",
    "def map_user_week(d_week, display_parent_res=8, min_dwell=10):\n",
    "    d = d_week.copy()\n",
    "\n",
    "    # parent aggregation for visibility\n",
    "    if display_parent_res is not None:\n",
    "        d[\"hex_show\"] = d[\"hex_id\"].astype(str).apply(lambda x: to_parent(x, display_parent_res))\n",
    "    else:\n",
    "        d[\"hex_show\"] = d[\"hex_id\"].astype(str)\n",
    "\n",
    "    # dwell by (hex,label)\n",
    "    g = d.groupby([\"hex_show\",\"label\"], as_index=False)[\"duration_min\"].sum()\n",
    "    mix = g.pivot_table(index=\"hex_show\", columns=\"label\", values=\"duration_min\", fill_value=0)\n",
    "\n",
    "    dwell_total = mix.sum(axis=1)\n",
    "    dom = mix.idxmax(axis=1)\n",
    "\n",
    "    span = d.groupby(\"hex_show\").agg(\n",
    "        earliest=(\"start_time\",\"min\"),\n",
    "        latest=(\"end_time\",\"max\"),\n",
    "        n=(\"hex_show\",\"size\")\n",
    "    )\n",
    "\n",
    "    summary = pd.DataFrame({\"dominant\": dom, \"dwell_total\": dwell_total}).join(span)\n",
    "    summary = summary[summary[\"dwell_total\"] >= min_dwell].copy()\n",
    "\n",
    "    if len(summary) == 0:\n",
    "        return folium.Map(location=[51.5,-0.1], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "    # center by top dwell hex\n",
    "    top_hex = summary[\"dwell_total\"].idxmax()\n",
    "    latc, lonc = h3.cell_to_latlng(top_hex) if hasattr(h3, \"cell_to_latlng\") else h3.h3_to_geo(top_hex)\n",
    "    m = folium.Map(location=[latc, lonc], zoom_start=13, tiles=\"CartoDB positron\")\n",
    "\n",
    "    # center marker\n",
    "    folium.CircleMarker(\n",
    "        location=[latc, lonc],\n",
    "        radius=5,\n",
    "        fill=True,\n",
    "        fill_opacity=0.9,\n",
    "        popup=\"Top dwell hex center\"\n",
    "    ).add_to(m)\n",
    "\n",
    "    all_points = []\n",
    "    for hx, row in summary.iterrows():\n",
    "        act = str(row[\"dominant\"])\n",
    "        col = ACT_COL.get(act, \"#777777\")\n",
    "\n",
    "        # top3 label breakdown for this hex\n",
    "        label_break = mix.loc[hx].sort_values(ascending=False)\n",
    "        top3 = label_break.head(3)\n",
    "        break_html = \"<br>\".join([f\"{k}: {v:.0f} min\" for k,v in top3.items() if v > 0])\n",
    "\n",
    "        tooltip = f\"{act} | {row['earliest']:%m-%d %H:%M}–{row['latest']:%m-%d %H:%M}\"\n",
    "        popup = (f\"<b>hex</b>: {hx}<br><b>dominant</b>: {act}<br>\"\n",
    "                 f\"<b>dwell_total</b>: {row['dwell_total']:.0f} min<br>\"\n",
    "                 f\"<b>top labels</b>:<br>{break_html}<br>\"\n",
    "                 f\"<b>span</b>: {row['earliest']:%Y-%m-%d %H:%M} – {row['latest']:%Y-%m-%d %H:%M}<br>\"\n",
    "                 f\"<b>stays</b>: {int(row['n'])}\")\n",
    "\n",
    "        coords = h3_to_geojson_polygon(hx)\n",
    "        for lon, lat in coords[0]:\n",
    "            all_points.append((lat, lon))\n",
    "\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\"color\": col},\n",
    "            \"geometry\": {\"type\": \"Polygon\", \"coordinates\": coords},\n",
    "        }\n",
    "\n",
    "        folium.GeoJson(\n",
    "            feature,\n",
    "            style_function=lambda feat: {\n",
    "                \"fillColor\": feat[\"properties\"][\"color\"],\n",
    "                \"color\": feat[\"properties\"][\"color\"],\n",
    "                \"weight\": 2,\n",
    "                \"fillOpacity\": 0.35\n",
    "            },\n",
    "            tooltip=folium.Tooltip(tooltip),\n",
    "            popup=folium.Popup(popup, max_width=360),\n",
    "        ).add_to(m)\n",
    "\n",
    "    if all_points:\n",
    "        lats = [p[0] for p in all_points]\n",
    "        lons = [p[1] for p in all_points]\n",
    "        m.fit_bounds([[min(lats), min(lons)], [max(lats), max(lons)]])\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bae4207-fe82-4184-886a-e6bb8a682ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3d15bb5228442397888d8577f32f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Horizon:', options=('1W', '1M', '3M'), value='1W'), Dropdown(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a1634690d846ffa3d06bd3b19a0627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Prev user', icon='arrow-left', style=ButtonStyle()), Button(description='Ne…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8ad77570a24b2a879851a22b092a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- Widgets ----------\n",
    "horizon_dd = widgets.Dropdown(options=[\"1W\",\"1M\",\"3M\"], value=\"1W\", description=\"Horizon:\")\n",
    "model_dd = widgets.Dropdown(options=available_models(\"1W\"), description=\"Model:\")\n",
    "\n",
    "group_options = [\"All users\"] + sorted(group_to_users.keys()) if group_to_users else [\"All users\"]\n",
    "group_dd = widgets.Dropdown(options=group_options, value=\"All users\", description=\"Sample:\")\n",
    "\n",
    "user_dd = widgets.Dropdown(options=[], description=\"User:\")\n",
    "week_dd = widgets.Dropdown(options=[], description=\"Week:\")\n",
    "\n",
    "btn_prev = widgets.Button(description=\"Prev user\", icon=\"arrow-left\")\n",
    "btn_next = widgets.Button(description=\"Next user\", icon=\"arrow-right\")\n",
    "\n",
    "out_box = widgets.Output()\n",
    "_lock = {\"busy\": False}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def refresh_model_options():\n",
    "    opts = available_models(horizon_dd.value)\n",
    "    model_dd.options = opts\n",
    "    if opts:\n",
    "        if model_dd.value not in opts:\n",
    "            model_dd.value = opts[0]\n",
    "    else:\n",
    "        model_dd.value = None\n",
    "\n",
    "def get_user_pool():\n",
    "    df = load_pred(horizon_dd.value, model_dd.value)\n",
    "    all_users = sorted(df[\"user_id\"].unique())\n",
    "\n",
    "    # Sample-pack filtering ONLY for 1W; for 1M/3M always show all users\n",
    "    if horizon_dd.value != \"1W\" or group_dd.value == \"All users\":\n",
    "        return all_users\n",
    "\n",
    "    pack_users = group_to_users.get(group_dd.value, [])\n",
    "    pool = [u for u in pack_users if u in set(all_users)]\n",
    "    return pool if pool else all_users\n",
    "\n",
    "def refresh_users():\n",
    "    if model_dd.value is None:\n",
    "        user_dd.options = []\n",
    "        user_dd.value = None\n",
    "        return\n",
    "\n",
    "    df = load_pred(horizon_dd.value, model_dd.value)\n",
    "    all_users = sorted(df[\"user_id\"].unique())\n",
    "\n",
    "    # apply sample-pack filter only for 1W\n",
    "    if horizon_dd.value == \"1W\" and group_dd.value != \"All users\":\n",
    "        pack_users = group_to_users.get(group_dd.value, [])\n",
    "        pool = [u for u in pack_users if u in set(all_users)]\n",
    "        if not pool:\n",
    "            pool = all_users\n",
    "    else:\n",
    "        pool = all_users\n",
    "\n",
    "    #  preserve current user if possible\n",
    "    current = user_dd.value\n",
    "    user_dd.options = pool\n",
    "    if current in pool:\n",
    "        user_dd.value = current\n",
    "    else:\n",
    "        user_dd.value = pool[0] if pool else None\n",
    "\n",
    "def refresh_weeks():\n",
    "    if model_dd.value is None or user_dd.value is None:\n",
    "        week_dd.options = []\n",
    "        week_dd.value = None\n",
    "        return\n",
    "\n",
    "    df = load_pred(horizon_dd.value, model_dd.value)\n",
    "    d = df[df[\"user_id\"] == user_dd.value].copy()\n",
    "    if len(d) == 0:\n",
    "        week_dd.options = []\n",
    "        week_dd.value = None\n",
    "        return\n",
    "\n",
    "    d[\"start_time\"] = to_local_time_series(d[\"start_time\"], tz=TZ_LONDON, assume_utc_if_naive=True)\n",
    "    ws = sorted(d[\"start_time\"].apply(week_start_monday_local).dropna().unique())\n",
    "    week_dd.options = [str(x) for x in ws]\n",
    "    week_dd.value = week_dd.options[0] if week_dd.options else None\n",
    "\n",
    "def goto_user(delta):\n",
    "    opts = list(user_dd.options)\n",
    "    if not opts or user_dd.value is None:\n",
    "        return\n",
    "    i = opts.index(user_dd.value)\n",
    "    j = max(0, min(len(opts)-1, i+delta))\n",
    "    user_dd.value = opts[j]\n",
    "\n",
    "btn_prev.on_click(lambda _: goto_user(-1))\n",
    "btn_next.on_click(lambda _: goto_user(+1))\n",
    "\n",
    "def render():\n",
    "    with out_box:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if model_dd.value is None or user_dd.value is None or week_dd.value is None:\n",
    "            print(\"No selection / no data.\")\n",
    "            return\n",
    "\n",
    "        df = load_pred(horizon_dd.value, model_dd.value)\n",
    "        d_week = filter_user_week(df, user_dd.value, week_dd.value, tz=TZ_LONDON)\n",
    "\n",
    "        print(f\"Horizon={horizon_dd.value} | Model={model_dd.value} | Sample={group_dd.value}\")\n",
    "        print(f\"User={user_dd.value} | Week={week_dd.value} | rows={len(d_week)}\")\n",
    "        \n",
    "        mid0 = d_week[(d_week[\"start_time\"].dt.hour==0) & (d_week[\"start_time\"].dt.minute==0)].copy()\n",
    "        print(\"Segments starting at 00:00 in this slice:\", len(mid0))\n",
    "        display(mid0[[\"start_time\",\"end_time\",\"duration_min\",\"hex_id\",\"label\"]].head(10))\n",
    "        \n",
    "        # quick sanity: list non-HOME stays in this week\n",
    "        non_home = d_week[d_week[\"label\"]!=\"HOME\"].copy().sort_values(\"start_time\")\n",
    "        print(\"\\nNon-HOME stays (top 15):\")\n",
    "        display(non_home[[\"start_time\",\"end_time\",\"duration_min\",\"hex_id\",\"label\"]].head(15))\n",
    "\n",
    "        nh, ent, sw = proxy_summary(d_week)\n",
    "        print(f\"Proxy: night_home_share={nh:.3f} | tophex_entropy={ent:.3f} | switch_rate(day-avg)={sw:.2f}\")\n",
    "\n",
    "        plot_week_timeline(d_week, title=f\"{model_dd.value} — timeline\")\n",
    "        m = map_user_week(d_week, display_parent_res=8, min_dwell=10)\n",
    "        display(m)\n",
    "\n",
    "def full_refresh():\n",
    "    if _lock[\"busy\"]:\n",
    "        return\n",
    "    _lock[\"busy\"] = True\n",
    "    try:\n",
    "        # if horizon != 1W, force sample to All users to avoid empty pool\n",
    "        if horizon_dd.value != \"1W\" and group_dd.value != \"All users\":\n",
    "            group_dd.value = \"All users\"\n",
    "\n",
    "        refresh_model_options()\n",
    "        refresh_users()\n",
    "        refresh_weeks()\n",
    "        render()\n",
    "    finally:\n",
    "        _lock[\"busy\"] = False\n",
    "\n",
    "# ---------- Wire events ----------\n",
    "def refresh_for_context_change():\n",
    "    refresh_model_options()\n",
    "    refresh_users()\n",
    "    refresh_weeks()\n",
    "    render()\n",
    "\n",
    "def on_user_change(_=None):\n",
    "    # user 改变时不要刷新 users 本身！\n",
    "    refresh_weeks()\n",
    "    render()\n",
    "\n",
    "def on_week_change(_=None):\n",
    "    render()\n",
    "\n",
    "# wiring\n",
    "horizon_dd.observe(lambda _: refresh_for_context_change(), names=\"value\")\n",
    "model_dd.observe(lambda _: refresh_for_context_change(), names=\"value\")\n",
    "group_dd.observe(lambda _: refresh_for_context_change(), names=\"value\")\n",
    "\n",
    "user_dd.observe(lambda _: on_user_change(), names=\"value\")\n",
    "week_dd.observe(lambda _: on_week_change(), names=\"value\")\n",
    "\n",
    "# ---------- Init ----------\n",
    "full_refresh()\n",
    "\n",
    "display(widgets.HBox([horizon_dd, model_dd, group_dd, user_dd, week_dd]))\n",
    "display(widgets.HBox([btn_prev, btn_next]))\n",
    "display(out_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5f7e5-00b9-4a5c-9537-1f50a857bdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d9bbd-646e-4bbb-89fe-20acd094c3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e68bf6d-5ac8-4646-8945-343067679e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added sample group: secondary_night_home_ge60min | n = 13\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# add an extra group: secondary-night-home users (>=60min) from notebook 08\n",
    "sec_path = OUT_TAB / \"uk_1w_secondary_night_home_users_ge60min.csv\"\n",
    "if sec_path.exists():\n",
    "    sec_users = pd.read_csv(sec_path)[\"user_id\"].astype(str).unique().tolist()\n",
    "    group_to_users[\"secondary_night_home_ge60min\"] = sorted(sec_users)\n",
    "    print(\"Added sample group:\", \"secondary_night_home_ge60min\", \"| n =\", len(sec_users))\n",
    "else:\n",
    "    print(\"No secondary-night-home list found at:\", sec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661305ca-44f8-4655-9f1c-2e9980fa73d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
